{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e18247-7607-4193-8090-998d8433926c",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/datasets/tongpython/cat-and-dog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6b86d-472d-4335-b387-346e85f249f3",
   "metadata": {},
   "source": [
    "## Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb81d835-b82e-4b6f-a167-118925f2dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c6411b-40fc-4519-b71a-614b2311d927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (cn1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (cn2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# LeNet was created by AT&T, named after Yann LeCun\n",
    "# Image size is resized to 32x32, as per CIFAR10 size\n",
    "# The first linear layer must be sized based on the size of the kernels, and the size of the images used to train/test after resizing\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 3 input image channel, 6 output feature maps and 5x5 conv kernel\n",
    "        self.cn1 = nn.Conv2d(3, 6, 5) # (32-5+0)/1 + 1 => 28 * 28 * 6\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        # After pooling, 28/2 => 14 * 14 * 6\n",
    "        # 6 input image channel, 16 output feature maps and 5x5 conv kernel\n",
    "        self.cn2 = nn.Conv2d(6, 16, 5) # (14-5+0)/1 + 1 = 10 * 10 * 16\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        # fully connected layers of size 120, 84 and 10\n",
    "        # After pooling, 10/2 => 5 * 5 * 16\n",
    "        self.flatten1 = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 is the spatial dimension at this layer\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution with 5x5 kernel\n",
    "        x = F.relu(self.cn1(x))\n",
    "        # Max pooling over a (2, 2) window        \n",
    "        x = self.maxpool1(x)\n",
    "        # Convolution with 5x5 kernel\n",
    "        x = F.relu(self.cn2(x))\n",
    "        # Max pooling over a (2, 2) window        \n",
    "        x = self.maxpool2(x)\n",
    "        # Flatten spatial and depth dimensions into a single vector        \n",
    "        x = self.flatten1(x)\n",
    "        # Fully connected operations\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def flattened_features(self, x):\n",
    "        # all except the first (batch) dimension\n",
    "        size = x.size()[1:]  \n",
    "        num_feats = 1\n",
    "        for s in size:\n",
    "            num_feats *= s\n",
    "        return num_feats\n",
    "\n",
    "# Making the code device-agnostic\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "lenet = LeNet()\n",
    "lenet.to(device)\n",
    "print(lenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d777327b-07aa-42c4-9a58-9da061248439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, optim, epoch):\n",
    "    # initialize loss\n",
    "    final_loss_total = 0.0\n",
    "    num_batches = 0\n",
    "    success = 0\n",
    "    counter = 0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_batches += 1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # ip refers to the input images, and ground_truth refers to the output classes the images belong to\n",
    "        ip, ground_truth = data\n",
    "        ip = ip.to(device)\n",
    "        ground_truth = ground_truth.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optim.zero_grad()\n",
    "\n",
    "        op = net(ip) # forward pass        \n",
    "        loss = nn.CrossEntropyLoss()(op, ground_truth) # get the loss\n",
    "        loss.backward() # use loss to perform a backward pass\n",
    "        optim.step() # optimisation step\n",
    "\n",
    "        _, pred = torch.max(op.data, 1) # Get the predictions for purpose of accuracy\n",
    "        counter += ground_truth.size(0)\n",
    "        success += (pred == ground_truth).sum().item()\n",
    "\n",
    "        # update loss\n",
    "        final_loss_total += loss.item()        \n",
    "\n",
    "    accuracy = 100 * success / counter\n",
    "    print('[Epoch number : %d] accuracy: %.3f, loss: %.3f' % (epoch + 1, accuracy, final_loss_total / num_batches))\n",
    "    return final_loss_total / num_batches, accuracy\n",
    "\n",
    "def test(net, testloader, train_test=\"test\"):\n",
    "    success = 0\n",
    "    counter = 0\n",
    "    num_batches = 0\n",
    "    loss_total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            num_batches += 1\n",
    "            im, ground_truth = data\n",
    "\n",
    "            im = im.to(device)\n",
    "            ground_truth = ground_truth.to(device)\n",
    "            \n",
    "            op = net(im)\n",
    "            _, pred = torch.max(op.data, 1)\n",
    "            loss = nn.CrossEntropyLoss()(op, ground_truth)\n",
    "            # update loss\n",
    "            loss_total += loss.item()\n",
    "            \n",
    "            counter += ground_truth.size(0)\n",
    "            success += (pred == ground_truth).sum().item()\n",
    "\n",
    "    accuracy = 100 * success / counter\n",
    "    print(f\"LeNet accuracy on {len(testloader.dataset)} images from {train_test} dataset: {accuracy}. Loss: {loss_total / num_batches}\")\n",
    "    # Return loss\n",
    "    return loss_total / num_batches, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f3932bc-3668-4e6d-96be-d220745e622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data items: 8005\n",
      "Number of test data items: 2023\n",
      "Number of cat labels in training: 4000\n",
      "Number of dog labels in training: 4005\n",
      "Number of cat labels in test: 1011\n",
      "Number of dog labels in test: 1012\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.Resize(size=(32,32), antialias=True),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=\"catdog/training_set\", transform=train_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(size=(32,32), antialias=True), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=\"catdog/test_set\", transform=test_transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "classes = ('cat', 'dog')\n",
    "\n",
    "# Print labels\n",
    "print(f\"Number of training data items: {len(train_dataset)}\") \n",
    "print(f\"Number of test data items: {len(test_dataset)}\") \n",
    "print(f\"Number of cat labels in training: {sum(1 for i in train_dataset.targets if i == 0)}\")\n",
    "print(f\"Number of dog labels in training: {sum(1 for i in train_dataset.targets if i == 1)}\")\n",
    "print(f\"Number of cat labels in test: {sum(1 for i in test_dataset.targets if i == 0)}\")\n",
    "print(f\"Number of dog labels in test: {sum(1 for i in test_dataset.targets if i == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23a600-29c5-45fd-9056-a9278fecc6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch number : 1] accuracy: 79.950, loss: 0.425\n",
      "\n",
      "LeNet accuracy on 2023 images from test dataset: 71.57686604053386. Loss: 0.5461070537567139\n",
      "\n",
      "[Epoch number : 2] accuracy: 80.949, loss: 0.417\n",
      "\n",
      "LeNet accuracy on 2023 images from test dataset: 73.40583292140386. Loss: 0.5322513580322266\n",
      "\n",
      "[Epoch number : 3] accuracy: 81.524, loss: 0.401\n",
      "\n",
      "LeNet accuracy on 2023 images from test dataset: 73.30696984676223. Loss: 0.5425006151199341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define optimizer\n",
    "optim = torch.optim.Adam(lenet.parameters(), lr=0.001)\n",
    "\n",
    "training_loss = []\n",
    "training_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "epochs = 10\n",
    "\n",
    "# training loop over the dataset multiple times\n",
    "for epoch in range(epochs):  \n",
    "    t_loss, t_acc = train(lenet, trainloader, optim, epoch)        \n",
    "    training_loss.append(t_loss)\n",
    "    training_acc.append(t_acc)\n",
    "    print() # Newline\n",
    "    v_loss, v_acc = test(lenet, testloader)\n",
    "    val_loss.append(v_loss)\n",
    "    val_acc.append(v_acc)\n",
    "    print()\n",
    "\n",
    "# Save model for future use\n",
    "model_path = './base_model_100_epoch.pth'\n",
    "torch.save(lenet.state_dict(), model_path)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0d193-ead8-4eab-9840-bf362502a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_index = range(1, epochs+1, 1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ax1.plot(x_index, training_loss, label=\"Training Loss\")\n",
    "ax1.plot(x_index, val_loss, color='red', label=\"Validation Loss\")\n",
    "ax1.set_title(\"Loss over time\")\n",
    "ax1.legend()\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "ax2.plot(x_index, val_acc, label=\"Validation Accuracy\")\n",
    "ax2.plot(x_index, training_acc, color='red', label=\"Training Accuracy\")\n",
    "ax2.legend()\n",
    "ax2.set_title(\"Accuracy Over Time\")\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f0ab5-77d2-406e-a381-b830dd9d2165",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        im, ground_truth = data\n",
    "\n",
    "        # Move tensors to GPU for doing model testing\n",
    "        im = im.to(device)\n",
    "        ground_truth = ground_truth.to(device)\n",
    "        \n",
    "        op = lenet(im)        \n",
    "        _, pred = torch.max(op.data, 1)\n",
    "        counter += ground_truth.size(0)\n",
    "        success += (pred == ground_truth).sum().item()\n",
    "\n",
    "print(f'Model accuracy on {len(testloader.dataset)} images from test dataset: %d %%' % (\n",
    "    100 * success / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8985ac-0b6c-4fe6-b85e-c241661a089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_success = list(0. for i in range(2))\n",
    "class_counter = list(0. for i in range(2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        im, ground_truth = data\n",
    "\n",
    "        # Move tensors to GPU for doing model testing\n",
    "        im = im.to(device)\n",
    "        \n",
    "        op = lenet(im)\n",
    "        _, pred = torch.max(op, 1)\n",
    "\n",
    "        # Move tensors back to CPU\n",
    "        pred = pred.cpu()\n",
    "        im = im.cpu()\n",
    "        ground_truth = ground_truth.cpu()\n",
    "        \n",
    "        c = (pred == ground_truth).squeeze()\n",
    "        for i in range(2023):\n",
    "            ground_truth_curr = ground_truth[i]\n",
    "            class_success[ground_truth_curr] += c[i].item()\n",
    "            class_counter[ground_truth_curr] += 1\n",
    "\n",
    "for i in range(2):\n",
    "    print('Model accuracy for class %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_success[i] / class_counter[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148b484-66bf-4a94-8a89-31c2078cffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = test(lenet, testloader)\n",
    "print(loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809cc31-007c-4362-8d8f-161b70ab2b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
